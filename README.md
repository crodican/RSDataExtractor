# RS Data Collector Tool

This Python application provides a graphical user interface (GUI) for scraping and organizing credential data related to Recovery Specialists (RS), Certified Family Recovery Specialists (CFRS), and Certified Recovery Support Specialists (CRSS) in Pennsylvania. The tool fetches data from a specified website, processes it, and generates output in CSV and Excel formats, including a filtered dataset for specific Pennsylvania counties.

The application is built using `PyQt5` for the GUI, `requests` for web scraping, `BeautifulSoup` for parsing HTML, and `pandas` for data manipulation and output.

## ‚ú® Features

* **User-Friendly GUI:** Simple and intuitive interface for data collection.

* **Credential-Specific Scraping:** Choose to extract data for CRS, CFRS, or CRSS.

* **Multi-threaded Operation:** Performs web scraping in a separate thread to keep the GUI responsive.

* **Progress Tracking:** Displays real-time progress of the scraping process.

* **Detailed Logging:** Provides an expandable log panel for detailed operation feedback.

* **Data Integration:** Merges scraped data with a pre-defined list of PA cities and counties.

* **Multiple Output Formats:** Generates:

  * A CSV file with all collected data for the selected credential in Pennsylvania.

  * A CSV file filtered to include only data from specific target counties.

  * An Excel workbook (`.xlsx`) containing two sheets: one with all PA data and one with the filtered county data.

* **Customizable Fonts & Styling:** Utilizes custom fonts for a polished look.

## üöÄ Installation

### Prerequisites

* Python 3.x

* Required Python packages: `PyQt5`, `requests`, `pandas`, `beautifulsoup4`, `openpyxl`

* `avenir.ttf`, `avenir-heavy.ttf`, `avenir-black.ttf` font files (These are expected to be in the same directory as the script or a subdirectory named `fonts/` if modified in `FONT_PATHS`).

### Steps

1. **Clone the repository (or download the script):**
`git clone https://github.com/your-username/rs-data-collector.git
cd rs-data-collector`

2. **Install Python dependencies:**

`pip install PyQt5 requests pandas beautifulsoup4 openpyxl`

3. **Place Font Files (Important):**
Ensure that the font files (`avenir.ttf`, `avenir-heavy.ttf`, `avenir-black.ttf`) are located in the same directory as the `main.py` (or whatever your main script is named) file, or update the `FONT_PATHS` dictionary in the script to reflect their correct location.

## üñ•Ô∏è How to Use

1. **Run the application:**
`python your_script_name.py`

(Replace `your_script_name.py` with the actual name of your Python script file, e.g., `main.py`)

2. **Select Data:**

* The application window will appear.

* Click one of the buttons (`CRS Data`, `CFRS Data`, or `CRSS Data`) to start the scraping process for the respective credential type.

3. **Monitor Progress:**

* A progress bar and status line will appear at the bottom of the window, indicating the scraping status.

* Click the "Show Output" button to expand a log console for more detailed information. You can click "Hide Output" to collapse it.

4. **Download Data:**

* Once the scraping is complete, a "Download Data" button will become visible.

* Click this button to choose a directory where you want to save the generated CSV and Excel files.

## ‚öôÔ∏è Project Structure (Key Components)

* **`main.py` (or your script name):**

* **`get_city_county_df(output_lines)`:** Fetches a CSV of PA cities and counties.

* **`get_total_pages(base_url, output_lines)`:** Determines the total number of pages to scrape from the target website.

* **`scrape_website(base_url, total_pages, output_lines, progress_callback)`:** Performs the actual web scraping, extracts data, and reports progress.

* **`Communicate` (QObject):** A helper class for emitting signals from the worker thread to update the GUI.

* **`RecoverySpecialistApp` (QWidget):** The main GUI class.

 * Initializes the application window, layout, and widgets.

 * Handles button clicks (`start_scrape`, `download_data`, `toggle_log`).

 * Manages the scraping process by spawning a `threading.Thread` (`scrape_worker`).

 * Connects signals from the worker thread to update the progress bar and log.

 * Manages UI state (button visibility, log expansion).

 * `build_stylesheet()` and `update_fonts()` manage the application's visual styling.

* **`scrape_worker(credential_choice)`:** The function executed in a separate thread. It orchestrates the data fetching, processing, merging, and file generation.

## üõ†Ô∏è Configuration

You can modify the following variables in the `main.py` script:

* **`COUNTY_URL`**: The URL for the PA cities and counties CSV file.

* **`HEADERS`**: HTTP headers used for web requests.

* **`FONT_PATHS`**: A dictionary mapping font names to their file paths. Ensure these paths are correct relative to the script or absolute.

* **`target_counties`**: (within `scrape_worker`) A list of specific Pennsylvania counties for which filtered data will be generated. You can customize this list to include or exclude counties.

## ü§ù Contributing

Feel free to fork this repository, submit pull requests, or open issues if you encounter any bugs or have suggestions for improvements.

## üìÑ License

This project is licensed under the MIT License - see the `LICENSE` file for details.
